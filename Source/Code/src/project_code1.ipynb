{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true,"id":"sqb4-8zByu1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","print(\"TensorFlow version:\", tf.__version__)\n","import tensorflow.keras as tfk\n","from tensorflow.keras.layers import Dense, Flatten, Conv1D, Embedding, Normalization, Conv1DTranspose,InputLayer\n","import tensorflow.keras.layers as tfkl\n","from tensorflow.keras import Model\n","import math\n","import time\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import librosa\n","import json\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import matplotlib.pyplot as plt\n","import tensorflow_io as tfio\n","import random\n","import tensorflow as tf\n","from pathlib import Path\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T06:50:21.266065Z","iopub.execute_input":"2022-05-12T06:50:21.266398Z","iopub.status.idle":"2022-05-12T06:50:21.275736Z","shell.execute_reply.started":"2022-05-12T06:50:21.266343Z","shell.execute_reply":"2022-05-12T06:50:21.275005Z"},"editable":false,"trusted":true,"id":"mxrm2Ng1yu1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('../input/birdclef-2022/scored_birds.json','r') as sb:\n","  s_b = json.load(sb)\n","file_path = '../input/birdclef-2022'\n","train_df = pd.read_csv('../input/birdclef-2022/train_metadata.csv')\n","train_df = train_df[train_df['primary_label'].isin(s_b)]\n","bird_label = train_df[\"primary_label\"].unique()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T06:50:22.548175Z","iopub.execute_input":"2022-05-12T06:50:22.548525Z","iopub.status.idle":"2022-05-12T06:50:22.688876Z","shell.execute_reply.started":"2022-05-12T06:50:22.548490Z","shell.execute_reply":"2022-05-12T06:50:22.687853Z"},"editable":false,"trusted":true,"id":"yu0cFl6iyu1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_peaks(y, sr, n_peaks=12, kernel_size=15, zero_dist=50, FMIN=500, FMAX=12500):\n","    melspec = librosa.feature.melspectrogram(y, sr=sr,\n","        fmin=FMIN, fmax=FMAX, n_mels=64)\n","    pcen = librosa.core.pcen(melspec, sr=sr,\n","        gain=0.8, bias=10, power=0.25, time_constant=0.06, eps=1e-06)\n","\n","    pcen_snr = np.max(pcen,axis=0) - np.min(pcen,axis=0)\n","    pcen_snr = librosa.power_to_db(pcen_snr / np.median(pcen_snr))\n","    median_pcen_snr = scipy.signal.medfilt(pcen_snr, kernel_size=kernel_size)\n","    # And go through, picking some peaks\n","    print(len(y))\n","    times = np.linspace(0, len(y)/sr, num=melspec.shape[1])\n","    peak_locs = []\n","    for i in range(n_peaks):\n","        t_peak = np.argmax(median_pcen_snr)\n","        peak_locs.append(times[t_peak])\n","        median_pcen_snr[t_peak-50:t_peak+50] = 0 # zero out around the peak to find the next one\n","\n","    return peak_locs\n","def sample_the_peak(peaks,y):\n","  peaks=[int(i) for i in peaks]\n","  peak_list=[]\n","  for i in range(len(peaks)):\n","    if (int((peaks[i]-2.5)*32000)<0) or (int((peaks[i]+2.5)*32000)>len(y)):\n","      continue \n","    else:\n","      peak_list.append(y[int((peaks[i]-2.5)*32000):int((peaks[i]+2.5)*32000)])\n","  return peak_list\n","#peaks=get_peaks(y, sr, n_peaks=5)\n","import scipy\n","from scipy.signal import bessel, freqz, lfilter\n","def filter_highpass(cutoff, fs, order=5):\n","    nyq = fs * 0.5\n","    normal_cutoff = cutoff / nyq\n","    b, a = bessel(order, normal_cutoff, btype='high', analog=False)\n","    return b, a\n","\n","def bessel_highpass_filter(data, cutoff, fs, order=5):\n","    b,a = filter_highpass(cutoff, fs, order)\n","    y  = lfilter(b, a, data)\n","    return y"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T06:50:24.954638Z","iopub.execute_input":"2022-05-12T06:50:24.955754Z","iopub.status.idle":"2022-05-12T06:50:24.971012Z","shell.execute_reply.started":"2022-05-12T06:50:24.955669Z","shell.execute_reply":"2022-05-12T06:50:24.970062Z"},"editable":false,"trusted":true,"id":"N8mhUuRxyu1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"editable":false,"id":"Jy-57iwfyu1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_path = '../input/birdclef-2022/train_audio'\n","\n","def preprocessing(df, path,bird_label):\n","  le = 160000\n","  step = int((le/2))\n","  sample_rate = 32000\n","  train = []\n","  c=0\n","  for label in tqdm(bird_label):\n","    files = librosa.util.find_files(os.path.join(path, label))\n","    c=c+1\n","    print(\"now_label:\",c)\n","    for f in tqdm(files):\n","      yi = np.where(bird_label == label)\n","      # load audio\\\n","      #print(\"1:\",type(yi),type(yi.shape),yi[:10])\n","      y, sr = librosa.load(f,sr=sample_rate)\n","      #print(y)\n","      y = ((y-np.amin(y))*2)/(np.amax(y) - np.amin(y)) - 1\n","      #print(\"2:\",type(y),y.shape,y[:10])\n","      # \n","      order = 6\n","      fs = 30.0       \n","      cutoff = 3.667\n","      y= bessel_highpass_filter(y,cutoff,fs,order=5)\n","      peak_locs=get_peaks(y, sr, n_peaks=12, kernel_size=15, zero_dist=50, FMIN=500, FMAX=12500)\n","      y1=sample_the_peak(peak_locs,y)\n","      #print(y,type(y))\n","      \"\"\"\n","      intervals = librosa.effects.split(y, top_db= 15, ref= np.max)\n","      intervals = intervals.tolist()\n","      print(\"3-1:\",type(y),y.shape,y[:10])\n","      \n","      y = (y.flatten()).tolist()\n","      print(\"3:\",type(y),y[:10]\n","      \n","      nonsilent_y = []\n","      \n","      for p,q in intervals :\n","       nonsilent_y = nonsilent_y + y[p:q+1] \n","      print(\"4:\",type(nonsilent_y),nonsilent_y[:10])\n","      y = np.array(nonsilent_y).astype('float32')\n","      if len(y) < le:\n","        while len(y) < le:\n","          y = np.concatenate((y, y))\n","        y = y[:le]\n","      print(\"5:\",type(y),y.shape,y[:10])\n","      \"\"\"\n","      \n","      # A 1024-point STFT with frames of 5 s and 50% overlap.\n","      data_collector,label_collector=[],[]\n","      #print(len(y1))\n","      for i in range(len(y1)):\n","          #print(\"-----\",y1[i],type(y1[i]),y1[i].shape)\n","          if i>5: break\n","          else: \n","            stfts = tf.signal.stft(y1[i], frame_length=le, frame_step=step,\n","                       fft_length=4096)\n","            #print(\"6:stfts\",type(stfts),stfts[:10])\n","            spectrograms = tf.abs(stfts)\n","            num_spectrogram_bins = stfts.shape[-1]\n","            #print(\"7: num_spectrograms\",type(num_spectrogram_bins),num_spectrogram_bins)\n","            lower_edge_hertz, upper_edge_hertz, num_mel_bins = 1000.0, 8000.0, 4096\n","            linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n","        num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n","        upper_edge_hertz)\n","            #print(\"8: linear_to_mel_spectorgrams_matrix\",type(linear_to_mel_weight_matrix),linear_to_mel_weight_matrix[:10],linear_to_mel_weight_matrix.shape)\n","     \n","            #print(\"9: spectorgrams_matrix\",type(spectrograms),spectrograms[:10],spectrograms.shape)\n","            spectrograms = tf.cast(spectrograms, tf.float32)\n","            mel_spectrograms = tf.tensordot(\n","        spectrograms, linear_to_mel_weight_matrix, 1)\n","            #print(\"9: mel_spectrograms\",type(mel_spectrograms),mel_spectrograms[:10])\n","        \n","            mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n","        linear_to_mel_weight_matrix.shape[-1:]))\n","            #print(\"continuous to mel spectrogema the shsape of the mel is \",mel_spectrograms.shape)\n","        \n","            # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n","            log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n","            #print(\"9: log_mel_spectrograms:\",type(log_mel_spectrograms),log_mel_spectrograms[:10])\n","    \n","      \n","            mfccs = tf.signal.mfccs_from_log_mel_spectrograms(\n","        log_mel_spectrograms)\n","            #print(\"final\",type(mfccs),mfccs.shape,mfccs[:10])\n","            data_collector.append(mfccs)\n","            label_collector.append(label)\n","            for i in data_collector:\n","                for j in i:\n","                    #print(\"train_len=\",len(train))\n","                    train.append((j, yi))\n","  return train"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T06:52:23.954162Z","iopub.execute_input":"2022-05-12T06:52:23.954497Z","iopub.status.idle":"2022-05-12T06:52:23.982368Z","shell.execute_reply.started":"2022-05-12T06:52:23.954462Z","shell.execute_reply":"2022-05-12T06:52:23.980952Z"},"editable":false,"trusted":true,"id":"b_eLvVTbyu1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = preprocessing(train_df, train_path, bird_label)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T06:52:24.603927Z","iopub.execute_input":"2022-05-12T06:52:24.604227Z","iopub.status.idle":"2022-05-12T07:07:47.211832Z","shell.execute_reply.started":"2022-05-12T06:52:24.604176Z","shell.execute_reply":"2022-05-12T07:07:47.210588Z"},"editable":false,"trusted":true,"id":"1Xu5R9payu1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_data)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:07:51.351404Z","iopub.execute_input":"2022-05-12T07:07:51.352002Z","iopub.status.idle":"2022-05-12T07:07:51.366870Z","shell.execute_reply.started":"2022-05-12T07:07:51.351942Z","shell.execute_reply":"2022-05-12T07:07:51.365278Z"},"editable":false,"trusted":true,"id":"cGptJO-gyu1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[10000][1]"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:09:20.322104Z","iopub.execute_input":"2022-05-12T07:09:20.322492Z","iopub.status.idle":"2022-05-12T07:09:20.331875Z","shell.execute_reply.started":"2022-05-12T07:09:20.322452Z","shell.execute_reply":"2022-05-12T07:09:20.331057Z"},"editable":false,"trusted":true,"id":"OkmDE41lyu1Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## "],"metadata":{"editable":false,"id":"rY0E7ebPyu1Z"}},{"cell_type":"code","source":["class TensorflowDataGenerator():\n","    'Characterizes a dataset for Tensorflow'\n","    def __init__(self, mel_list, batch_size):\n","      self.mel_list = mel_list\n","      self.batch_size = batch_size\n","      self.index_helper = 0\n","      self.le = len(mel_list)\n","    def __len__(self):\n","        return math.ceil(self.le/ self.batch_size)\n","\n","    def __getitem__(self, index):\n","      if self.index_helper >= self.le:\n","        raise IndexError\n","      x, y = [], []\n","      for b in range(self.batch_size):\n","        if self.index_helper < self.le:\n","          #print(\"shape of original mel_list\",mel)\n","          x.append(tf.expand_dims(self.mel_list[self.index_helper][0],0))\n","          y.append(tf.squeeze(self.mel_list[self.index_helper][1]))\n","          self.index_helper += 1\n","          \n","      return np.array(x).astype('float32'), np.array(y).astype('float32')\n","\n","    def reset(self):\n","      self.index_helper = 0\n","        "],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:23:04.794409Z","iopub.execute_input":"2022-05-12T07:23:04.795051Z","iopub.status.idle":"2022-05-12T07:23:04.807504Z","shell.execute_reply.started":"2022-05-12T07:23:04.794988Z","shell.execute_reply":"2022-05-12T07:23:04.806334Z"},"editable":false,"trusted":true,"id":"CzVIDd_tyu1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.seed(2022)\n","random.shuffle(train_data)  # shuffle it randomly\n","\n","training_data = train_data[:int(0.9*len(train_data))]\n","val_data = train_data[int(0.9*len(train_data)):]\n","\n","batch_size = 32\n","\n","\n","train_set = TensorflowDataGenerator(training_data,batch_size)\n","\n","\n","val_set = TensorflowDataGenerator(val_data,batch_size)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:23:05.174289Z","iopub.execute_input":"2022-05-12T07:23:05.174659Z","iopub.status.idle":"2022-05-12T07:23:05.207216Z","shell.execute_reply.started":"2022-05-12T07:23:05.174622Z","shell.execute_reply":"2022-05-12T07:23:05.206436Z"},"editable":false,"trusted":true,"id":"kCqjukvyyu1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dense, Dropout\n","from tensorflow.keras.layers import AvgPool1D, GlobalAveragePooling1D, MaxPool1D, Conv1DTranspose\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import ReLU, concatenate\n","import tensorflow.keras.backend as K\n","# Creating Densenet121\n","tf.random.set_seed(2022)\n","def densenet(input_shape, n_classes, filters = 32):   \n","    #batch norm + relu + conv\n","    def bn_rl_conv(x,filters,kernel=1,strides=1):\n","        x = BatchNormalization()(x)\n","        x = ReLU()(x)\n","        x = Conv1D(filters, kernel, strides=strides,padding = 'same')(x)\n","        x = Dropout(0.1)(x)\n","        return x\n","    \n","    def dense_block(x, repetition):\n","        \n","        for _ in range(repetition):\n","            y = bn_rl_conv(x, 4*filters)\n","            y = bn_rl_conv(y, filters, 3)\n","            x = concatenate([y,x])\n","        return x\n","        \n","    def transition_layer(x):\n","        \n","        x = bn_rl_conv(x, K.int_shape(x)[-1] //2 )\n","        x = AvgPool1D(2, strides = 2, padding = 'same')(x)\n","        return x\n","    \n","    input = Input (input_shape)\n","    x = Conv1D(64, 3, strides=1, padding='causal', dilation_rate = 2, activation = 'relu')(input)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(64, 3, strides=1, padding='causal', dilation_rate = 4, activation = 'relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(64, 3, strides=1, padding='causal', dilation_rate = 8, activation = 'relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(64, 7, strides = 2, padding = 'same')(x)\n","    x = Conv1DTranspose(32, 3,strides=1, activation = 'relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1DTranspose(64, 3,strides=1, activation = 'relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1DTranspose(128, 3,strides=1, activation = 'relu')(x)\n","    x = BatchNormalization()(x)\n","    x = MaxPool1D(3, strides = 2, padding = 'same')(x)\n","    \n","    for repetition in [6,12,32,32]:\n","        \n","        d = dense_block(x, repetition)\n","        x = transition_layer(d)\n","    x = GlobalAveragePooling1D()(d)\n","    x = Dense(2048 , activation = 'relu',kernel_regularizer=tf.keras.regularizers.L1(0.01),\n","    activity_regularizer=tf.keras.regularizers.L2(0.01))(x)\n","    x = Dropout(0.25)(x)\n","    output = Dense(n_classes, activation = 'softmax')(x)\n","    model = Model(input, output)\n","    return model\n","input_shape = (1, 4096)\n","n_classes = 21\n","model = densenet(input_shape,n_classes)\n","# [6,12,32,32]:"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:23:05.494852Z","iopub.execute_input":"2022-05-12T07:23:05.495435Z","iopub.status.idle":"2022-05-12T07:23:10.996760Z","shell.execute_reply.started":"2022-05-12T07:23:05.495381Z","shell.execute_reply":"2022-05-12T07:23:10.995459Z"},"editable":false,"trusted":true,"id":"kHSbiqWqyu1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# learning_rate=1e-4 Adadelta\n","optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n","epoches = 9\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","train_acc = tf.keras.metrics.Mean()\n","train_loss = tf.keras.metrics.Mean()\n","val_acc = tf.keras.metrics.Mean()\n","val_loss = tf.keras.metrics.Mean()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:23:10.999214Z","iopub.execute_input":"2022-05-12T07:23:10.999592Z","iopub.status.idle":"2022-05-12T07:23:11.021659Z","shell.execute_reply.started":"2022-05-12T07:23:10.999545Z","shell.execute_reply":"2022-05-12T07:23:11.020267Z"},"editable":false,"trusted":true,"id":"5AaIcSmayu1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(x_batch, y_batch):\n","  with tf.GradientTape() as tape:\n","    logits = model(x_batch, training=True)\n","    loss_value = loss_fn(y_batch, logits)\n","    grads = tape.gradient(loss_value, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","    acc_value = tf.math.equal(y_batch, tf.cast(tf.math.argmax(logits, 1),dtype=tf.float32))\n","    train_acc.update_state(acc_value)\n","    train_loss.update_state(loss_value)\n","    \n","def val_step(x_batch_val, y_batch_val):\n","\n","  val_logits = model(x_batch_val, training=False)\n","  loss_value = loss_fn(y_batch_val,val_logits) # check input and ground truth shape \n","\n","  acc_value = tf.math.equal(y_batch_val, tf.cast(tf.math.argmax(val_logits, 1),dtype=tf.float32))\n","  val_acc.update_state(acc_value)\n","  val_loss.update_state(loss_value)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:23:11.024484Z","iopub.execute_input":"2022-05-12T07:23:11.025057Z","iopub.status.idle":"2022-05-12T07:23:11.035311Z","shell.execute_reply.started":"2022-05-12T07:23:11.025011Z","shell.execute_reply":"2022-05-12T07:23:11.034655Z"},"editable":false,"trusted":true,"id":"5EdzTno-yu1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(epoches):\n","  if epoch == 2:\n","    optimizer.lr.assign(1e-6)\n","  elif epoch == 3:\n","    optimizer.lr.assign(1e-5)\n","  elif epoch == 5:\n","    optimizer.lr.assign(1e-6)\n","  start_time = time.time()\n","  train_set.reset()\n","  val_set.reset()\n","  for x_batch, y_batch in tqdm(train_set):\n","    train_step(x_batch, y_batch)\n","    \n","  for x_batch_val, y_batch_val in tqdm(val_set):\n","    val_step(x_batch_val, y_batch_val)\n","  end_time = time.time()\n","  print(f'Epoch: {epoch} \\tTraining Loss: {train_loss.result()} \\tValidation Loss: {val_loss.result()} \\tTraining Accuracy: {train_acc.result()} \\tValidation Accuracy: {val_acc.result()} \\tTime taken: {end_time - start_time}')\n","\n","    \n","  train_acc.reset_states()\n","  train_loss.reset_states()\n","  val_acc.reset_states()\n","  val_loss.reset_states()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:23:11.037085Z","iopub.execute_input":"2022-05-12T07:23:11.038015Z"},"editable":false,"trusted":true,"id":"6kLGDkVuyu1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def test_step(x_batch_val):\n","  val_logits = model(x_batch_val, training=False)\n","  return tf.math.argmax(val_logits,1)"],"metadata":{"editable":false,"trusted":true,"id":"muihA3ccyu1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_path = '../input/birdclef-2022/test_soundscapes/'\n","test_files = os.listdir(test_path)\n","def preprocessing_test_dat(test_path, files):\n","  le = 160000\n","  step = int((le/2))\n","  sample_rate = 32000\n","  test = []\n","  for file in tqdm(files):\n","    y, sr = librosa.load(test_path + file, sr=sample_rate)\n","    # y = y[:le + 1]\n","    for segment in range(0, len(y), sample_rate*5):\n","        row_id = file[:-4] + '_' + str(int((segment + (sample_rate * 5)) / (sample_rate)))\n","        if segment+le > len(y):\n","            yi = y[segment:]\n","            while len(yi) < le:\n","              yi = np.concatenate((yi, yi))\n","            yi = yi[:le]\n","        else:\n","            yi = y[segment:segment+le]\n","            \n","        stfts = tf.signal.stft(yi, frame_length=le, frame_step=le,\n","                       fft_length=4096)\n","        spectrograms = tf.abs(stfts)\n","\n","        # Warp the linear scale spectrograms into the mel-scale.\n","        num_spectrogram_bins = stfts.shape[-1]\n","        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 1000.0, 8000.0, 4096\n","\n","        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n","        num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n","        upper_edge_hertz)\n","      \n","        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n","        mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n","          linear_to_mel_weight_matrix.shape[-1:]))\n","\n","        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n","        log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n","  \n","        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)\n","        test.append((row_id, mfccs))\n","  return test"],"metadata":{"editable":false,"id":"DW2Ht4yCyu1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TensorflowDataGenerator_test():\n","    'Characterizes a dataset for Tensorflow'\n","    def __init__(self, mel_list, batch_size):\n","      self.mel_list = mel_list\n","      self.batch_size = batch_size\n","      self.index_helper = 0\n","      self.le = len(mel_list)\n","    def __len__(self):\n","        return math.ceil(self.le/ self.batch_size)\n","\n","    def __getitem__(self, index):\n","      if self.index_helper >= self.le:\n","        raise IndexError\n","      x, y = [], []\n","      for b in range(self.batch_size):\n","        if self.index_helper < self.le:\n","          x.append(self.mel_list[self.index_helper][0])\n","          y.append(self.mel_list[self.index_helper][1])\n","          self.index_helper += 1\n","          \n","      return x, np.array(y).astype('float32')\n","\n","    def reset(self):\n","      self.index_helper = 0\n","        "],"metadata":{"editable":false,"id":"8pMg-YWGyu1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dat = preprocessing_test_dat(test_path, test_files)"],"metadata":{"editable":false,"id":"N7_Wcc3Nyu1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","test_set = TensorflowDataGenerator_test(test_dat,batch_size)"],"metadata":{"editable":false,"id":"utjnE27nyu1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = []\n","test_set.reset()\n","for x_batch, y_batch in tqdm(test_set):\n","    preds = test_step(y_batch)\n","    for idx, pred in enumerate(preds):\n","        split_code = x_batch[idx].split('_')\n","        for bird in bird_label:\n","            row_id = split_code[0] +'_'+ split_code[1]+'_' + bird+'_'+split_code[2]\n","            predictions.append([row_id, True if bird == bird_label[pred] else False])"],"metadata":{"editable":false,"id":"sOsvEO01yu1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_df = pd.DataFrame(predictions,columns=['row_id', 'target'])\n","sub_df.head()"],"metadata":{"editable":false,"id":"x-ofAXOIyu1d"},"execution_count":null,"outputs":[]}]}